---
title: Assessing Model Accuracy (Linear Models)
author: Giuliano Sposito
date: '2018-01-09'
categories:
  - data science
tags:
  - evalutation
  - model
  - rstat
slug: assessing-model-accuracy-linear-modelsRmd
thumbnailImagePosition: left
thumbnailImage: images/green_flames_digital_art_lines.png
disqusIdentifier: rstatmodelaccuracy
---



<p>This post talks about the use of <strong>Mean Squared Error (MSE)</strong> against the flexibility of a function fitted as a technique to assess the model accuracy in a specific problem as describe in the <a href="https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370">An Introduction to Statistical Learning in R</a> book.</p>
<!--more-->
<div id="measure-the-quality-of-fit-regression-problems" class="section level2">
<h2>Measure the quality of fit (regression problems)</h2>
<p>In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.</p>
<p>In the <strong>regression setting</strong>, the most commonly-used measure is the mean squared error (MSE), given by:</p>
<p><span class="math display">\[ MSE = \frac{1}{N}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2 \]</span></p>
<p>Where <span class="math inline">\(\hat{f}(x_i)\)</span> is the predicted (or fitted) function at x<sub>i</sub> and y<sub>i</sub> is the real value.</p>
<p>So, the MSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE, but we want to evaluate the performance of the <span class="math inline">\(\hat{f}()\)</span> against the unknown data points, so we also compute MSE in an <em>test set</em> with data points different from used to fit the <span class="math inline">\(\hat{f}()\)</span>, now we have a MSE<sub>tr</sub> for training points and a MSE~ts for test set.</p>
<p>We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE (MSE<sub>ts</sub>).</p>
</div>
<div id="comparing-msetr-and-msets" class="section level2">
<h2>Comparing MSE<sub>tr</sub> and MSE<sub>ts</sub></h2>
<p>Let’s simulate some situations to see how MSE<sub>tr</sub> and MSE~ts against different fitting techniques, we’ll use polynomials fit to simplify the scenarios.</p>
<div id="curve-1" class="section level3">
<h3>Curve 1</h3>
<pre class="r"><code># setup
library(ggplot2)
library(tidyverse)
library(reshape2)

set.seed(42)</code></pre>
<pre class="r"><code># full domain of points (continuous from 0 to 100)
DOMAIN &lt;- 0:100

# function linear gausian noise sd=1
f &lt;- function(x) 0.0005*x^2 + 0.05*x + 0.5
noise &lt;- function(x) 0.5 * rnorm(x)

# build the datasete
data_frame(
  x = DOMAIN, 
  f = f(x) # the &#39;real value&#39;
) %&gt;%
  # adding noise
  mutate(
    y = f + noise(DOMAIN) # adding some noise
  ) -&gt; dt

# separing in training and testing
idx.tr &lt;- sample(DOMAIN,round(length(DOMAIN)/2))
dt_tr &lt;- dt[idx.tr,]
dt_ts &lt;- dt[-idx.tr,]

#  visualizing training data
ggplot(dt_tr, aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=f), linetype=&quot;dotted&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/caseOne-1.png" width="672" /></p>
<p>Let’s fit some cases in these data sets, we will use an linear regression, and some polynomial data.</p>
<pre class="r"><code># lets fit linear model and polynomials with degree 2, 3, e 10
degrees &lt;- c(1,2,3,5,10)

# a function to fit a poly
fitPoly &lt;- function(degree,data){
  lm(y ~ poly(x, degree, raw=TRUE), data)
}

# apply the functions on the selected degrees
models &lt;- map(degrees, fitPoly, data=dt_tr)

# lets get de predicted values for these models
models %&gt;%
  map(function(model){model$fitted.values}) %&gt;%
  set_names(paste0(&quot;f&quot;,degrees)) %&gt;%
  as_data_frame() %&gt;%
  cbind(dt_tr, .) -&gt; dt_tr_fit

# converting from wide to long format to plot all together
dt_tr_long &lt;- dt_tr_fit %&gt;%
  melt(id.vars=&quot;x&quot;, variable.name=&quot;model&quot;, value.name = &quot;fitted&quot;)

# ploting the fitted curves
ggplot(dt_tr_long) +
  geom_line(data=dt_tr_long[dt_tr_long$model!=&quot;y&quot;,], aes(x=x, y=fitted, colour=model)) +
  geom_point(data=dt_tr_long[dt_tr_long$model==&quot;y&quot;,], aes(x=x,y=fitted)) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/fitCaseOne-1.png" width="672" /></p>
<p>We see in this chart, the real data points (points), the real function (continuous black line) and different fitting curves (colored lines) from 1 degree to 50 degree. Now let’s see the performances of these models, calculating and plotting MSE on training and testing sets.</p>
<pre class="r"><code># calc MSE from the residuals of the model
getMSE &lt;- function(lm.model) sum(lm.model$residuals^2)/length(lm.model$residuals)

# calc MSE to the training set in a model
calcMSE &lt;- function(lm.model, newdata){
  y_hat &lt;- predict(lm.model, newdata=newdata)
  mse &lt;- (1/length(y_hat))*sum( (newdata$y-y_hat)^2 )
  return(mse)
}

# performances
perf &lt;- data_frame(
  degree = degrees,
  MSE.tr = unlist(map(models, getMSE)),
  MSE.ts = unlist(map(models, calcMSE, dt_ts))
)

# &quot;the real MSE&quot; inputed by noise
MSE &lt;- sum( (dt$y-dt$f)^2 ) / nrow(dt)

# plot the performances
ggplot(perf,aes(x=degree)) +
  geom_line(aes(y=MSE.tr), colour=&quot;red&quot;) +
  geom_line(aes(y=MSE.ts), colour=&quot;blue&quot;) +
  geom_hline(yintercept = MSE, linetype=&quot;dashed&quot;) +
  ylab(&quot;MSE&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/perfCaseOne-1.png" width="672" /></p>
<p>We can see the behavior of MSE data, in the training data (Red) the increasing of the flexibility of the fit (degree in this case) will cause a continuous decreasing in the MSE value, but in the MSE of the test data we have a initial decreasing until some minimal value (the optimal fit) and then a increasing, showing that model over fitting the training set.</p>
</div>
<div id="curve-2" class="section level3">
<h3>Curve 2</h3>
<p>Another example.</p>
<pre class="r"><code># now the domains is 100 random
DOMAIN &lt;- 0:100

# function linear gausian noise sd=1
f &lt;- function(x) (-sin( (2*pi/length(DOMAIN)) * (x+10) )) * 2*x/100 + 0.001 * x
noise &lt;- function(x) 0.3*rnorm(x)

# the dataset
data_frame(
  x = DOMAIN,
  f = f(x) # &quot;real value&quot;
) %&gt;%
  # adding noise
  mutate(
    y = f + noise(DOMAIN) # with noise
  ) -&gt; dt

# separing in training and testing
idx.tr &lt;- sample(DOMAIN,round(length(DOMAIN)/2))
dt_tr &lt;- dt[idx.tr,]
dt_ts &lt;- dt[-idx.tr,]

#  visualizing training data
ggplot(dt_tr, aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=f), linetype=&quot;dotted&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/caseTwo-1.png" width="672" /></p>
<pre class="r"><code># degress to fit
degrees &lt;- c(1,2,3,5,10)

# fit the models
models &lt;- map(degrees, fitPoly, data=dt_tr)

# get fitted values
models %&gt;%
  map(function(model){model$fitted.values}) %&gt;%
  set_names(paste0(&quot;f&quot;,degrees)) %&gt;%
  as_data_frame() %&gt;%
  cbind(dt_tr, .) -&gt; dt_tr_fit

# from wide to long
dt_tr_long &lt;- dt_tr_fit %&gt;%
  melt(id.vars=&quot;x&quot;, variable.name=&quot;model&quot;, value.name = &quot;fitted&quot;)

# plot the fitted values
ggplot(dt_tr_long) +
  geom_line(data=dt_tr_long[dt_tr_long$model!=&quot;y&quot;,], aes(x=x, y=fitted, colour=model)) +
  geom_point(data=dt_tr_long[dt_tr_long$model==&quot;y&quot;,], aes(x=x,y=fitted)) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/fitCaseTwo-1.png" width="672" /></p>
<pre class="r"><code># performances
perf &lt;- data_frame(
  degree = degrees,
  MSE.tr = unlist(map(models, getMSE)),
  MSE.ts = unlist(map(models, calcMSE, dt_ts))
)

# &quot;the real MSE&quot; inputed by noise
MSE &lt;- sum( (dt$y-dt$f)^2 ) / nrow(dt)

# plot the MSEs
ggplot(perf,aes(x=degree)) +
  geom_line(aes(y=MSE.tr), colour=&quot;red&quot;) +
  geom_line(aes(y=MSE.ts), colour=&quot;blue&quot;) +
  geom_hline(yintercept = MSE, linetype=&quot;dashed&quot;) +
  ylab(&quot;MSE&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/perfCaseTwo-1.png" width="672" /></p>
</div>
</div>
<div id="curve-3" class="section level1">
<h1>Curve 3</h1>
<pre class="r"><code>DOMAIN &lt;- runif(100, 1, 100)

# function linear gausian noise sd=1
f &lt;- function(x) -30*sin( x*(2*pi/length(DOMAIN)) ) - .01*x^2 + 15   #  (sin( (2*pi/length(DOMAIN)) * (x+10) )) * 2*x/100 + 0.001 * x
noise &lt;- function(x) 5*rnorm(x)

data_frame(
  x = DOMAIN,
  f = f(x)
) %&gt;%
  # adding noise
  mutate(
    y = f + noise(DOMAIN)
  ) -&gt; dt

# separing in training and testing
idx.tr &lt;- sample(DOMAIN,round(length(DOMAIN)/2))
dt_tr &lt;- dt[idx.tr,]
dt_ts &lt;- dt[-idx.tr,]

#  visualizing training data
ggplot(dt_tr, aes(x=x)) +
  geom_point(aes(y=y)) +
  geom_line(aes(y=f), linetype=&quot;dotted&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/caseThree-1.png" width="672" /></p>
<pre class="r"><code>degrees &lt;- c(1,2,3,5,10)

models &lt;- map(degrees, fitPoly, data=dt_tr)

models %&gt;%
  map(function(model){model$fitted.values}) %&gt;%
  set_names(paste0(&quot;f&quot;,degrees)) %&gt;%
  as_data_frame() %&gt;%
  cbind(dt_tr, .) -&gt; dt_tr_fit

dt_tr_long &lt;- dt_tr_fit %&gt;%
  melt(id.vars=&quot;x&quot;, variable.name=&quot;model&quot;, value.name = &quot;fitted&quot;)

ggplot(dt_tr_long) +
  geom_line(data=dt_tr_long[dt_tr_long$model!=&quot;y&quot;,], aes(x=x, y=fitted, colour=model)) +
  geom_point(data=dt_tr_long[dt_tr_long$model==&quot;y&quot;,], aes(x=x,y=fitted)) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/fittCaseThree-1.png" width="672" /></p>
<pre class="r"><code># performances
perf &lt;- data_frame(
  degree = degrees,
  MSE.tr = unlist(map(models, getMSE)),
  MSE.ts = unlist(map(models, calcMSE, dt_ts))
)

MSE &lt;- sum( (dt$y-dt$f)^2 ) / nrow(dt)

ggplot(perf,aes(x=degree)) +
  geom_line(aes(y=MSE.tr), colour=&quot;red&quot;) +
  geom_line(aes(y=MSE.ts), colour=&quot;blue&quot;) +
  geom_hline(yintercept = MSE, linetype=&quot;dashed&quot;) +
  ylab(&quot;MSE&quot;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-01-15-assessing-model-accuracy-linear-models_files/figure-html/perfCaseThree-1.png" width="672" /></p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>As we see, plotting the <strong>cost function</strong> (MSE in these cases) of fitted models against Training and Test data is helpful to check the sanity of your model to avoid the over fitting effect.</p>
<p>In these examples we study how the MSE vs Model Flexibility (degrees in the polynomial fitting) but we can study the cost function vs number of features, number of samples and others aspects of you problem domain, this is a know technique to check for over fitting in machine learning projects.</p>
</div>
